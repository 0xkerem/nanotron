#!/bin/bash
#SBATCH --job-name=doremi_training
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1          # crucial - only 1 task per dist per node!
#SBATCH --gres=gpu:4
#SBATCH --exclusive
#SBATCH --partition=hopper-prod
#SBATCH -o /fsx/phuc/slurm_logs/doremi/train_doremi_simple-%x-%j-train.out
#SBATCH --qos=high

echo "START TIME: $(date)"

USE_FAST=1 CUDA_DEVICE_MAX_CONNECTIONS=1 torchrun --nproc_per_node=4 examples/doremi/train_doremi.py --config-file examples/doremi/config_tiny_llama.yaml

echo "END TIME: $(date)"
